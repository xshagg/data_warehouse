# Project: Data Warehouse

A music streaming service Sparkify whants to move their data, stored in S3 as JSON files, onto the cloud. The task is to
build ETL pipeline to load files from S3 and store it to staging tables on Redshift.

## Source datasets

Source files are stored in S3 buckets:

* **Song data:** s3://udacity-dend/song_data
    * Each file is in JSON format
    * Each file contains metadata about a song and the artist of that song.
    * The files are partitioned by the first three letters of each song's track ID. Example:
      ```
      song_data/A/B/C/TRABCEI128F424C983.json
      song_data/A/A/B/TRAABJL12903CDCF1A.json
      ```

* **Log data:** s3://udacity-dend/log_data
    * Each file is in JSON format
    * The log files generated by [event simulator](https://github.com/Interana/eventsim) based on the songs in the Songs dataset
    * The log files are partitioned by year and month. Example:
    ```
    log_data/2018/11/2018-11-12-events.json
    log_data/2018/11/2018-11-13-events.json
    ```

## Redshift database

### Staging tables
Staging tables are used to direct copy from JSON files before transform and insert into the final tables

* **staging_songs** - songs' and artists' metadata
* **staging_events** - activity logs

### Fact Table

* **songplays** - records in event data associated with song plays i.e. records with page 'NextSong'

### Dimension Tables

* **users** - users in the app
* **songs** - songs in music database
* **artists** - artists in music database
* **time** - timestamps of records in songplays broken down into specific units

## Setup

Before running ETL process, we need to set up a Redshift cluster.

1. In the Redshift console create a new **IAM user** and assign AdministratorAccess
2. On the final stage copy access and secret keys into **AWS** section in `dwh.cfg`
3. Adjust values in the DWH section DB_* values in the CLUSTER section depending on your needs
4. Run `python3 create_cluster.py` to create the cluster
5. Run `python3 cluster_status.py` several times until the cluster is become **Available**
6. Copy **Endpoint** and **ARN** values into `dwh.cfg`
7. Run `python3 open_incoming_port.py` to allow incoming connections

Now the cluster is ready. The final step is to create tables.
```
python3 create_tables.py
```
This script may be run again any time you need to reset the database

To start ETL process, run
```
python3 etl.py
```

## Destroy the cluster
When all the work is done and cluster is no more needed, run the following command to destroy the cluster
```
pytho3 destroy_cluster.py
```